{"cells":[{"cell_type":"markdown","metadata":{"id":"mNMzLr2jhqdu"},"source":["\n","# Lab session 4: Transformer models\n","\n","\n","This lab covers sequence to sequence modeling with Transformer models.  It was designed to give you some first practical experience with Transformers, and we have limited the required amount of input, in order to keep the time and effort for this lab within limits.\n","\n","General instructions:\n","- Complete the code where needed\n","- Provide answers to questions only in the cell where indicated\n","- **Do not alter the evaluation cells** (`## evaluation`) in any way as they are needed for the partly automated evaluation process\n"]},{"cell_type":"markdown","metadata":{"id":"iH4Dyj82dESf"},"source":["# **Section 1: Introduction to HuggingFace and Basic Usage of Transformers**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NJJU4-hxfy-f"},"source":["We will use Transformer neural networks and explore their capabilities on some popular NLP tasks."]},{"cell_type":"markdown","metadata":{"id":"lB6tDuoKuk5Q"},"source":["## Huggingface\n","For this lab session, we’ll use [Huggingface's](https://huggingface.co/) library to build a encoder-decoder architecuture. Huggingface provides a quick way to use pre-trained and transformers-based NLP models. [BERT](https://huggingface.co/transformers/model_doc/bert.html), [T5](https://huggingface.co/transformers/model_doc/t5.html), [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html), [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) and many others are readily available in this library. \n","\n","Install the `transformers` and `sentencepiece` (required for tokenization) libraries:\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"On0RNwgQWV4T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806366340,"user_tz":-120,"elapsed":9460,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"335465e0-27be-48e2-9cb6-aca7c456f7a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.5.1\n","  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 4.0 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.95\n","  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 55.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.7.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.21.6)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 45.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (21.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0cb883c9c6e691281464f03023f5cd458afa386950f78883a9ec04e639d68fb3\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.95 tokenizers-0.10.3 transformers-4.5.1\n"]}],"source":["!pip install transformers==\"4.5.1\" sentencepiece==\"0.1.95\""]},{"cell_type":"markdown","metadata":{"id":"CVH8HszOTlIj"},"source":["### Usage\n","HuggingFace's Transformers library is built around three types of classes for each pretrained model:\n","\n","* **model** classes, e.g., `BertModel` which inherits `torch.nn.Modules` and handles loading pretrained weights.\n","\n","* **configuration** classes which store all the parameters required to build a model, e.g., `BertConfig`. You don’t always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).\n","\n","* **tokenizer** classes which store the vocabulary for each model and provide methods for encoding (and decoding) strings into a list of token embedding indices to be fed to a model, e.g., `BertTokenizer`.\n","\n","All these classes can be instantiated from pretrained instances and saved locally using two methods: \n","\n","1. `from_pretrained()` lets you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself or stored locally.\n","\n","2. `save_pretrained()` lets you save a model/configuration/tokenizer locally so that it can be reloaded using `from_pretrained()`.\n","\n","\n","For example you can load a pretrained model/config/tokenizer with:\n","\n","  ```\n","  # import library\n","  from transformers import BertModel, BertConfig, BertTokenizer\n","\n","  # load config\n","  configuration = BertConfig.from_pretrained('bert-base-uncased')\n","  \n","  # load tokenizer\n","  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","  \n","  # load model\n","  model = BertModel.from_pretrained(\"bert-base-uncased\")\n","  # or \n","  model = BertModel.from_pretrianed(configuration)\n","  ```\n","\n","Note that in this session we will focus on using and finetuning a pretrained model, not the (pre)training itself. "]},{"cell_type":"markdown","metadata":{"id":"AnxXjt1Lesv8"},"source":["### Question-1\n","\n","- What's the difference between pretraining and finetuning?   \n","\n","**<font color=blue><<< Pretrainig is a process to train a model and save it; finetuning is to put another set of data into the pretrained model, and make some adjustments according to project requirements. >>></font>**"]},{"cell_type":"markdown","metadata":{"id":"8Q7yfDBuQT0s"},"source":["We will use a pretrained T5 model to perform some initial experiments. As introduced in the theory lecture, T5 is a transformer based model which uses the encoder-decoder structure. It uses the same basic architecture as proposed in the original transformer paper [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762) with some minor variations. It is based on the core idea that most problems in NLP can be formulated as text to text transformation. In other words, given a sequence of words as input, the\n","model produces another sequence of words as output. The figure below shows how the input and output are formulated for performing a variety of NLP tasks using the T5 model (also see DL lecture 8).\n","\n","\n","<img src=\"https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif\">"]},{"cell_type":"markdown","metadata":{"id":"LBuB_t-FgLOv"},"source":["Lets see how T5 actually works. As always, we import the necessary modules and initialize with a specific random seed (for reproducibility). Your device should be set to \"cuda\", not \"cpu\". (If not, you can change this in \"Edit\" > \"Notebook settings\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"C7lyeDmLvH6k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806373970,"user_tz":-120,"elapsed":3040,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"8b996726-f2f2-49be-9527-c22d9a5281b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["# import necessary libraries\n","\n","import torch\n","import transformers\n","\n","import random\n","import numpy as np\n","\n","# for reproducibility\n","\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"4PijHm2EiTAW"},"source":["## Tokenizer playground\n","\n","To build an encoder-decoder pipeline we should prepare input data. We shall use the `tokenizer` class, which offers a clean way to convert raw text into ids. For this part, we'll ask you to:\n","\n","- load the `T5` tokenizer\n","- tokenize the given sentence into subwords (e.g., `*love NLP*` will convert to `['▁love', '▁N', 'LP']` according to the pretrained tokenizer)\n","- encode and then decode the given sentence  \n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1CmEj7FoRbN5","colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["2ff0243ee93e4281975d82bd8d5e1f45","4e81b69386904bf0b524cfde0694a47d","0d1cb03c92074f20a063206567d301f4","730bed6d91c7453ba0a9cab522c8f9ae","bd3eca54ffbc4e6399fbbbc6bdc0c17e","74c027a3b84a4f7383fcab4c5c863a00","a99b6a70217f4dc99698f10d761f158c","8f3e1bf4a10241e1a8accc8c58924281","9b56e2aa1577477eac7e8dfd15b998f9","a1bd6c20dac14528b689e06a9658a801","79734ffc76554622bad1b4713b7d749b","64722ef0fb81494a9e0d641d424c6c5f","daf3b2e4b5c64d379e5b1816fb19e74b","4028821f2d814c6ca690694ec9ae3a8c","d3f8a9af5a824677b01258daa403d99d","1f0d142d69094a14bf4e0a3060310dd0","88d250bc7a7346eb80a2b842ff4e1120","a9ed962539034f8da10189802d1eb533","56e97bc6d1ec40cd979cbfd5278c77bf","499bac713ed7441394562639f07d0486","3c2493b8fd254543aaec98cad3356b2c","f9192b497a894270844e66679d2cda3f"]},"executionInfo":{"status":"ok","timestamp":1652806382130,"user_tz":-120,"elapsed":4746,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"8a1f813e-35d7-4509-fd96-b346f1f3c7da"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff0243ee93e4281975d82bd8d5e1f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64722ef0fb81494a9e0d641d424c6c5f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['▁Don', \"'\", 't', '▁you', '▁love', '▁the', '▁N', 'LP', '▁course', '?', '▁We', '▁sure', '▁do', '.']\n","----------------------------------------------------------------------------------------------------\n","{'input_ids': tensor([[1008,   31,   17,   25,  333,    8,  445, 6892,  503,   58,  101,  417,\n","          103,    5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","----------------------------------------------------------------------------------------------------\n","Don't you love the\n"]}],"source":["from transformers import T5Tokenizer\n","\n","dummy_sentence = \"Don't you love the NLP course? We sure do.\"\n","\n","# 1) load t5 tokenizer (with `T5Tokenizer.from_pretrained', based on the \"t5-base\")\n","\n","# 2) tokenize dummy_sentence into subwords (use `tokenizer.tokenize')\n","# dummy_tokens = ...\n","\n","# 3) encode dummy_sentence into a pytorch tensor (use `tokenizer.encode_plus' with the argument return_tensors='pt', \n","# to return torch.Tensor objects). You can also just `__call__` the tokenizer.\n","# dummy_tensor = ...\n","\n","# 4) decode the first 6 input_ids [0,6) from the encoded input again (use `tokenizer.decode')\n","# dummy_decode = ...\n","\n","############### for student ################\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","dummy_tokens = tokenizer.tokenize(dummy_sentence)#将句子转化成subword\n","dummy_tensor = tokenizer.encode_plus(dummy_sentence, return_tensors='pt')\n","dummy_decode = tokenizer.decode(dummy_tensor['input_ids'][0][0:6])\n","\n","############################################\n","\n","print(dummy_tokens)\n","print('-' * 100)\n","print(dummy_tensor)\n","print('-' * 100)\n","print(dummy_decode)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-78ALsAtyQL"},"outputs":[],"source":["## evaluation\n","## DON'T CHANGE THIS CELL IN ANY WAY\n","\n","assert tokenizer is not None\n","assert tokenizer.name_or_path.find('t5')!=-1, \"load t5 tokenizer\"\n","assert len(tokenizer) == 32100, \"load base tokenizer\"\n","assert len(dummy_tokens) == 14\n","assert dummy_tokens[4] == '▁love'\n","assert isinstance(dummy_tensor, transformers.tokenization_utils_base.BatchEncoding), 'use encode_plus!'\n","assert dummy_decode == \"Don't you love the\"\n","\n","del dummy_tokens\n","del dummy_decode\n","del dummy_tensor\n","\n","print('Well done!')"]},{"cell_type":"raw","metadata":{"id":"59JujQ-MEeIl"},"source":["## Model playground\n","\n","In previous lab session, we created an embedding layer and used it to extract text features. We can apply the same idea in the context of pretrained transformers by loading a model and applying a classifier on top (cf. lecture on contextual representations). This enables us to adopt a pretrained model for our own use case. \n","\n","Assume we want to use T5's Encoder (`T5EncoderModel`, i.e., only the encoder from the full encoder-decoder model) for a binary classification task. Complete the following module to allow such binary classification using a simple linear layer.\n","\n","We will not finetune the model at this point, that's for further down the lab. "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"FC3qi_CeZkIa","executionInfo":{"status":"ok","timestamp":1652806382354,"user_tz":-120,"elapsed":1,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}}},"outputs":[],"source":["from torch import nn\n","from torch.nn import CrossEntropyLoss\n","from transformers import T5EncoderModel\n","\n","class BinaryClassifierWithT5(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # We only load encoder part\n","        # (ignore the warning message)\n","        self.t5_model = T5EncoderModel.from_pretrained('t5-base')\n","        \n","        # 1) Get the output dimension of the T5-base model. Huggingface refers \n","        #    to this dimension as `d_model`. You can either look up its value\n","        #    online (https://huggingface.co/t5-base/blob/main/config.json), \n","        #    or get it via `self.t5_model.config.d_model`.\n","        # t5_output_dim = ...\n","\n","        # 2) Create the linear layer with input dimension = t5_output_dim and a scalar output\n","        # self.classifier_head = ... (use a linear layer: `nn.Linear`)\n","        ############### for student ################\n","        t5_output_dim = self.t5_model.config\n","        self.classifier_head = nn.Linear(t5_output_dim.d_model, 1)\n","        ############################################\n","\n","\n","    def forward(self, input_ids=None, attention_mask=None):\n","        # The T5 model outputs a sequence of vectors of size `t5_output_dim`\n","        # (one vector for each token). The dimensions of this tensor are:\n","        # <batch_size, sequence length, t5_output_dim>.\n","        sequence_output = self.t5_model(input_ids, attention_mask)['last_hidden_state']\n","        # print(sequence_output)\n","        # 1) To end up with one vector for each sentence in the batch,\n","        #    we want to average the embeddings over all tokens.\n","        # averaged_sequence_output = ...  (use `torch.mean`)\n","\n","        # 2) Pass the averaged sentence embeddings through the linear layer\n","        # lm_logits = ...  (use `self.classifier_head(...)`)\n","        \n","        ############### for student ################\n","        averaged_sequence_output = torch.mean(sequence_output,1)\n","        # print(averaged_sequence_output.shape)\n","        lm_logits = self.classifier_head(averaged_sequence_output)\n","\n","        ############################################\n","\n","        return lm_logits\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bGaZ7Yfuy3Lz","colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["fcc9fbeac1774e5788449087996bb93b","556d88337ebf47c6939f39fafc2c8b5f","9735eb661a494868a3c65efa6f9ec592","36debd85e189454689022c786eab150b","2e5a2de569824df890b42f8586944561","daea43cc82134905b155521451a6f73c","b4c5d6591d1e4cf5979089e0a53e6820","c15f9021ecd64129816580b7935dc430","51287e1de7474431a1281a86467d31f0","7b7caf06b34c49d1a24ed4fc018733d6","03aa010034c241688d806ed5428454ce","b2cdb1623e394172b1f7b92215f780c1","93381ac540bd4aa1acb9eaa76fb387fc","bcb7c7ef800d45918e38b6c169c6e76b","0804817deb024aa5bd4d9c331d1cbbb4","fd58912a55cd48738797bfaff53ca88f","a36a45cd932c47aba445c14c923e20c4","c3a7cdc2f6164f5ca8d88fb68f2f4fc2","83c16027a3a24c2ab10567ffe7700267","9c13d2cc0abe4848a815723207cc8407","f0a0a52336864b4ea3bae90363dfbc45","af205d2db912485a922b1a4ce04f1b77"]},"executionInfo":{"status":"ok","timestamp":1652806407301,"user_tz":-120,"elapsed":22295,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"e43a327f-27f5-42fd-8ee5-8d7b66173e9d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcc9fbeac1774e5788449087996bb93b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2cdb1623e394172b1f7b92215f780c1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at t5-base were not used when initializing T5EncoderModel: ['decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight']\n","- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Well done!\n"]}],"source":["## evaluation\n","## DON'T CHANGE THIS CELL IN ANY WAY\n","\n","dummy_model = BinaryClassifierWithT5()\n","\n","dummy_inps = tokenizer.encode_plus(\"This is a simple example\", return_tensors='pt')\n","\n","dummy_output = dummy_model(input_ids=dummy_inps['input_ids'], attention_mask=dummy_inps['attention_mask'])\n","\n","assert isinstance(dummy_model.classifier_head, nn.Linear) \n","assert dummy_model.classifier_head.out_features == 1, 'Is it binary?'\n","\n","\n","del dummy_model\n","del dummy_inps\n","del dummy_output\n","\n","print('Well done!')"]},{"cell_type":"markdown","metadata":{"id":"-jydRRk4xbaU"},"source":["## Prompt or task specification\n","\n","Before jumping into the main part of this lab, we should be familar with Prompt. The technique of prompt or task specification is a way to steer the generation of pretrained language models to solve a (natural language) query of your choice. For example, [T. Brown et al.](https://arxiv.org/pdf/2005.14165.pdf) used prompting for grammar correction (the task of correcting different kinds of errors in text such as spelling, punctuation). They gave prompts of the form \"`Poor English Input: <inp_sentence>\\n Good English Output: <out_sentence>\"`:\n","\n","<img src=\"https://www.dropbox.com/s/ezfh1p891h7qes6/Screenshot%20from%202021-05-01%2009-40-00%20%28edited-Pixlr%29.png?raw=1\">\n","\n","\n","In this scenario, the encoder recieves a sentence in the form of \"`Poor English Input: <inp_sentence>\\n` and the decoder predicts the `Good English Output: <out_sentence>` with `<x_sentence>` an example in our dataset. \n","\n","\n","T5 has some built-in prompts such as:\n","\n","- translate English to French: `YOUR_INPUT_SENTENCE`\n","- translate English to German: `YOUR_INPUT_SENTENCE`\n","- cola sentence: `YOUR_INPUT_SENTENCE`\n","- ...\n","\n","Let's see how we can use the T5 model to translate \"`I am a student`\" into French and German using prompts in combination with a pretrained language model."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"LpiSZeCG42wH","executionInfo":{"status":"ok","timestamp":1652806413587,"user_tz":-120,"elapsed":6290,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}}},"outputs":[],"source":["from transformers import T5ForConditionalGeneration\n","\n","# load t5 model\n","t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"x8swr60OsYYE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806413804,"user_tz":-120,"elapsed":220,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"32ca09f2-98bd-4d73-de59-73c0f57ca249"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[13959,  1566,    12,  2379,    10,    27,   183,     3,     9,  1236,\n","             1]])\n","tensor([[    0,  1022,  3448,    73,     3, 24045,     1]])\n","Je suis un étudiant\n","['Je suis un étudiant']\n","tensor([[13959,  1566,    12,  2968,    10,    27,   183,     3,     9,  1236,\n","             1]])\n","tensor([[   0, 1674, 2701, 6341,   77,    1]])\n","Ich bin Studentin\n","['Je suis un étudiant', 'Ich bin Studentin']\n"]}],"source":["examples = [\n"," \"translate English to French: I am a student\", \n"," \"translate English to German: I am a student\",         \n","]\n","\n","translation_list = []\n","\n","# for each example:\n","# 1. encode your inputs and return a tensor with `tokenizer.encode`\n","# 2. pass the encoded input through the T5 model with the `generate` function\n","# 3. decode the generated output with the tokenizer (convert ids to tokens) with `tokenizer.decode`.\n","#    make sure to retain only the translation itself, not the special tokens such as padding\n","# 4. append this decoded output to the translation_list\n","\n","for e in examples:\n","    ############### for student ################\n","    encoding = tokenizer.encode(e,return_tensors='pt')\n","    print(encoding)\n","    generator = t5_model.generate(encoding) \n","    print(generator)\n","    decoding = tokenizer.decode(generator[0])\n","    translation = \" \".join(decoding.split(\" \")[1:]).replace(\"</s>\",\"\")\n","    print(translation)\n","    translation_list.append(translation)\n","    print(translation_list)\n","\n","\n","\n","\n","    ############################################\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"beVNPdPspQ9C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806414139,"user_tz":-120,"elapsed":337,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"6c82ef39-4db2-4660-d62f-e36957d2d095"},"outputs":[{"output_type":"stream","name":"stdout","text":["Well done!\n"]}],"source":["## evaluation\n","## DON'T CHANGE THIS CELL IN ANY WAY\n","assert len(translation_list) == 2, \"decode both examples?\"\n","assert translation_list[0] == \"Je suis un étudiant\"\n","assert translation_list[1] == \"Ich bin Studentin\"\n","\n","print('Well done!')"]},{"cell_type":"markdown","metadata":{"id":"1paas2u_y_Kk"},"source":["# **Section 2: Fine-tuning pretrained DistilBERT for classification**\n","\n","\n","In this experiment we'll fine-tune a pretrained DistilBERT model for a classification task. \n","DistilBERT is a small, fast, cheap and light Transformer model based on the BERT architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of the original BERT model by 40%. Here's an interesting [blog](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f) behind the approach of DistilBERT and knowledge distillation in BERT-like models in general. \n","\n","We use a twitter dataset of complaints of airline customers to build our classifiers."]},{"cell_type":"markdown","metadata":{"id":"K09zV32rpzQT"},"source":["### Question-2\n","\n","- What's the difference between finetuning and freezing transformers?   \n","\n","**<font color=blue><<< Weights of pretrain layers are to be updated in finetuning, and two learning rates are set, with the smaller LR applied to pretrain layers and the larger LR to classifier layer; freezing will only train the classifier layer let alone the pretrain layers. >>></font>**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OWmZLgLRphz6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806414140,"user_tz":-120,"elapsed":6,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"fa73e8db-978c-406a-c9d3-2744a00a1840"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","import torch\n","\n","print(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"h-vIfA96t9Vz","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1652806427048,"user_tz":-120,"elapsed":12911,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"28daca03-d6da-44ee-a5be-d0eab60b5d46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Collecting datasets\n","  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n","\u001b[K     |████████████████████████████████| 342 kB 3.2 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 56.5 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.8 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 73.3 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 85.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 86.4 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 57.3 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 72.3 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.1 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.6.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["urllib3"]}}},"metadata":{}}],"source":["!pip install Sentencepiece\n","!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"1js4xubKsca_","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1652806427576,"user_tz":-120,"elapsed":534,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"97c56819-7708-4ac5-f2ac-adcb5ecff326"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id                                              tweet  label\n","291    95181  @JLJeffLewis @AmericanAir no excuse for lost l...      0\n","755    20845  I thought airport wi-fi was ridiculous until I...      0\n","2096   32473  @DanniAllen14 @united @RunLikeAGirl_ca @just_t...      1\n","432   165082  My @united flight to LA had no electricity for...      0\n","479    37552  Poop. _@stevethebikeguy: @JetBlue announces ne...      0"],"text/html":["\n","  <div id=\"df-d02e157b-426b-4a7f-9862-f173ddf824ff\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>291</th>\n","      <td>95181</td>\n","      <td>@JLJeffLewis @AmericanAir no excuse for lost l...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>755</th>\n","      <td>20845</td>\n","      <td>I thought airport wi-fi was ridiculous until I...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2096</th>\n","      <td>32473</td>\n","      <td>@DanniAllen14 @united @RunLikeAGirl_ca @just_t...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>432</th>\n","      <td>165082</td>\n","      <td>My @united flight to LA had no electricity for...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>37552</td>\n","      <td>Poop. _@stevethebikeguy: @JetBlue announces ne...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d02e157b-426b-4a7f-9862-f173ddf824ff')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d02e157b-426b-4a7f-9862-f173ddf824ff button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d02e157b-426b-4a7f-9862-f173ddf824ff');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["complaint_train = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/twitter_train.csv\", encoding='latin-1')\n","complaint_test = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/twitter_test.csv\", encoding='latin-1')\n","\n","complaint_train.sample(5)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"gZDYQN9KqnMT","executionInfo":{"status":"ok","timestamp":1652806427577,"user_tz":-120,"elapsed":11,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}}},"outputs":[],"source":["X = complaint_train.tweet.values\n","y = complaint_train.label.values\n","\n","X_train, X_val, y_train, y_val =\\\n","    train_test_split(X, y, test_size=0.1, random_state=42)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"mHP3-vfYqtxt","executionInfo":{"status":"ok","timestamp":1652806427578,"user_tz":-120,"elapsed":12,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}}},"outputs":[],"source":["def preprocess_tweet(text):\n","  \n","    text = re.sub(r'(@.*?)[\\s]', ' ', text)    # Remove '@name'\n","    text = re.sub(r'&amp;', '&', text)  # Replace '&amp;' with '&'\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Remove trailing whitespace\n","\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"2ni_obYQ2RDo"},"source":["First, we need to tokenize the input text into token IDs, before it can be fed into DistilBERT. The figure below illustrates the tokenization process.\n","\n","\n","\n","![img](http://jalammar.github.io/images/distilBERT/bert-distilbert-input-tokenization.png)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"-AxSe0P_q08z","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["c59038882bc54e34b27bedc89a6e4758","1231241dce604afc833d50e9cf7342a7","7f011f2613814adda6a64ea06063165f","e571934030154e60ad64bbf6a2d3b02c","1d44dfbcb06b4da28204fab7dea99d2b","2b9e4ee1a38844c5bfe9df6badf7b16d","d29babd6b336433caae00485927caa77","9e4ae5baef7e44e19ca0a2c3639bfd78","2d9a682f0fb64fe58c6979ee5582326d","f46c5800af1644e2b18447a51320fa67","47b220a627094340b0c1bcc66b03e0a2","604988535b8c49f6ab111c1496bc224f","d8186aba6bf14030840a0418b11bad26","9b1f151bccd64b658fd5a643c8a00c31","479c7b25af1f4082bb9314d1464bc0a6","27d16b49d22e4e92bfa03043850d52e4","d603ac677adb487a93e6790de65aa00e","2adb709cf55443398770778797eb487d","5351bc2c2552476aade6b753037fd29d","6b1ea86698ff44178f6141d2496fa847","10322bbda984487d94461eed640b56e6","346beed676ea49e78776a32afa5fc5f7","e4d0c1c6aeb6483581a4cbc452eda020","776338a155e942bc924cb0cd9d1a987c","9f5e7b7351284727ac09908c4f99f24d","bd88e217aafd48cfa075a08fcf4d7e58","39b1486749f448cf92f06f722cb95d4f","ee3137fb71994aa0a647a9b664730e44","fb3e9da826794a5b8f4f980c4e6dfe33","cfee85d9796449d99be7a050f8229773","78b4863e77c64df7b3cfa392e2018f82","6fea7c17ee18439183c5fec3c1207aec","3768ad8d4f384d33ad6ddfc8eb476ef9"]},"executionInfo":{"status":"ok","timestamp":1652806429024,"user_tz":-120,"elapsed":1457,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"a3b0e8ee-d0ed-490c-bdbd-04ea5eaaff0e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c59038882bc54e34b27bedc89a6e4758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604988535b8c49f6ab111c1496bc224f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4d0c1c6aeb6483581a4cbc452eda020"}},"metadata":{}}],"source":["from transformers import DistilBertTokenizer, DistilBertModel\n","\n","# Load the Distilled BERT tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n","\n","\n","MAX_LEN = 64\n","\n","# Create a function to tokenize a set of texts\n","def preprocessing_for_bert(data):\n","    \"\"\"Perform required preprocessing steps for pretrained DistilBERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","\n","    for sent in data:\n","        # `tokenizer` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Return an attention mask\n","        #    (6) Return a dictionary of tokens mapped to IDs\n","        \n","\n","        encoded_sent = tokenizer(\n","            text=preprocess_tweet(sent),           # Preprocess the tweet\n","            add_special_tokens=True,               # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                    # Max length to truncate/pad\n","            padding='max_length',                  #Pad each sequence to the max_length argument provided        \n","            truncation =True,                      #Truncate each sequence to the max_length argument provided\n","            return_attention_mask = True\n","            )\n","\n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        \n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","    \n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"v4_OJWhiq_rL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806431053,"user_tz":-120,"elapsed":2033,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"06707807-65b5-494c-c515-cc1e1bc9d876"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing data...\n"]}],"source":["print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)\n"]},{"cell_type":"markdown","metadata":{"id":"FDLCutcRrPyN"},"source":["Now, we will create a Pytorch DataLoader. This allows us to easily load in batches of our new tokenized dataset during the training and validation process. "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"pBNQH2Wwq_w_","executionInfo":{"status":"ok","timestamp":1652806431054,"user_tz":-120,"elapsed":8,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}}},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"Z10ZEcWerZVg"},"source":["Next, we need to define the architecture of our classifier, which is built upon DistilBert. Please follow the instructions in the code below to add a feed-forward classifier after the pre-trained BERT model. You are also instructed to freeze the parameters in the pre-trained BERT model we have loaded from the transformer class. This ensures that only the newly defined classification layer is trained, while the parameters of the pre-trained BERT model are kept constant. "]},{"cell_type":"code","execution_count":17,"metadata":{"id":"voYvcSB7rdJA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652806431054,"user_tz":-120,"elapsed":7,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"dc147dcc-697b-4b5e-e33d-efcbcd549a4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 38 µs, sys: 6 µs, total: 44 µs\n","Wall time: 49.1 µs\n"]}],"source":["%%time\n","import torch\n","import torch.nn as nn\n","#from transformers import BertModel, DistilBertModel\n","from transformers import DistilBertTokenizer, DistilBertModel\n","\n","\n","# Create the DistilBertClassfier class\n","class DistilBertClassifier(nn.Module):\n","\n","    def __init__(self, unfreeze_layers=None):\n","        super(DistilBertClassifier, self).__init__()\n","\n","\n","        D_in, H, D_out = 768, 50, 2          # Specify hidden size of DistilBERT, hidden size of our classifier, and number of labels\n","\n","        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")    # Load DistilBERT model\n","        \n","        # Instantiate a one-layer feed-forward classifier\n","        # this classifier consists of a single hidden layer, with nn.RELU() between the hidden and output layer\n","        # self.classifier = nn.Sequential ( ... )\n","            \n","        ############### for student ################\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in,H),\n","            nn.Linear(H,D_out),\n","            nn.ReLU())\n","\n","        ############################################\n","\n","        # Freeze all the trainable layers in the DistilBertModel\n","        # (1) loop through all the parameters in self.bert (you should look up how to access the parameters of a layer/model in PyTorch)\n","        # (2) for each parameter, set requires_grad to False \n","\n","        ############### for student ################\n","        for param in self.bert.parameters():\n","            param.requires_grad = False\n","\n","        ############################################\n","\n","        # unfreeze/train the specific layers of the transformer \n","        if unfreeze_layers is not None:\n","            assert isinstance(unfreeze_layers, list), \"unfreeze_layers expects a list of layers to unfreeze\"\n","          \n","            for layer_no in unfreeze_layers:\n","                for param in list(self.bert.transformer.layer[layer_no].parameters()):\n","                    param.requires_grad = True\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \n","        outputs = self.bert(input_ids=input_ids,            # input_ids.shape = attention_mask.shape (batch_size, max_length)\n","                            attention_mask=attention_mask)\n","             \n","        last_hidden_state_cls = outputs[0][:, 0, :]         # Extract the last hidden state of the token `[CLS]` as an input for the classification task    \n","        logits = self.classifier(last_hidden_state_cls)     #logits.shape (batch_size, num_labels)\n","\n","        return logits\n"]},{"cell_type":"markdown","metadata":{"id":"p1jZSkH8r_i7"},"source":["We have implemented the training loop for you. Please study the code in the trainer class so you understand what is going on."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"56SOdsy0sB9i","executionInfo":{"status":"ok","timestamp":1652806431055,"user_tz":-120,"elapsed":6,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}}},"outputs":[],"source":["import random\n","import time\n","import torch.nn as nn\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def mytrainer(model, optimizer, train_dataloader,  val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the  model.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","   \n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        t0_epoch, t0_batch = time.time(), time.time()\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","        \n","        model.train()   # Put the model into the training mode\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            \n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)  # Load batch to GPU            \n","            model.zero_grad()    # Zero out any previously calculated gradients\n","            logits = model(b_input_ids, b_attn_mask)  # Perform a forward pass. \n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            loss.backward()    # Perform a backward pass to calculate gradients\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n"," \n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # Measure model's performance on the validation set after each epoch\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"Eval {epoch_i + 1:^2} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance on the validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"3bbxmIUpsLaD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652807347796,"user_tz":-120,"elapsed":53500,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"ba34ba54-feec-417d-e21c-f8ceb0d885ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   1    |   20    |   0.695315   |     -      |     -     |   1.28   \n","   1    |   40    |   0.693005   |     -      |     -     |   1.19   \n","   1    |   60    |   0.693103   |     -      |     -     |   1.19   \n","   1    |   80    |   0.691736   |     -      |     -     |   1.20   \n","   1    |   95    |   0.686611   |     -      |     -     |   0.89   \n","----------------------------------------------------------------------\n","Eval 1  |    -    |   0.692267   |  0.695928  |   48.07   |   6.40   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   0.685056   |     -      |     -     |   1.27   \n","   2    |   40    |   0.680827   |     -      |     -     |   1.22   \n","   2    |   60    |   0.682268   |     -      |     -     |   1.22   \n","   2    |   80    |   0.676365   |     -      |     -     |   1.22   \n","   2    |   95    |   0.686301   |     -      |     -     |   0.89   \n","----------------------------------------------------------------------\n","Eval 2  |    -    |   0.681978   |  0.681152  |   65.97   |   6.46   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   3    |   20    |   0.678084   |     -      |     -     |   1.29   \n","   3    |   40    |   0.676927   |     -      |     -     |   1.22   \n","   3    |   60    |   0.668648   |     -      |     -     |   1.22   \n","   3    |   80    |   0.672723   |     -      |     -     |   1.22   \n","   3    |   95    |   0.659312   |     -      |     -     |   0.89   \n","----------------------------------------------------------------------\n","Eval 3  |    -    |   0.671827   |  0.672001  |   61.31   |   6.49   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   4    |   20    |   0.669257   |     -      |     -     |   1.28   \n","   4    |   40    |   0.665132   |     -      |     -     |   1.22   \n","   4    |   60    |   0.659979   |     -      |     -     |   1.22   \n","   4    |   80    |   0.661744   |     -      |     -     |   1.24   \n","   4    |   95    |   0.664648   |     -      |     -     |   0.89   \n","----------------------------------------------------------------------\n","Eval 4  |    -    |   0.664179   |  0.665478  |   63.01   |   6.52   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   5    |   20    |   0.666575   |     -      |     -     |   1.32   \n","   5    |   40    |   0.657693   |     -      |     -     |   1.25   \n","   5    |   60    |   0.645933   |     -      |     -     |   1.26   \n","   5    |   80    |   0.633656   |     -      |     -     |   1.24   \n","   5    |   95    |   0.629954   |     -      |     -     |   0.93   \n","----------------------------------------------------------------------\n","Eval 5  |    -    |   0.647844   |  0.623977  |   69.03   |   6.66   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   6    |   20    |   0.631648   |     -      |     -     |   1.32   \n","   6    |   40    |   0.617136   |     -      |     -     |   1.25   \n","   6    |   60    |   0.624081   |     -      |     -     |   1.25   \n","   6    |   80    |   0.618063   |     -      |     -     |   1.25   \n","   6    |   95    |   0.618891   |     -      |     -     |   0.90   \n","----------------------------------------------------------------------\n","Eval 6  |    -    |   0.622225   |  0.603541  |   68.30   |   6.64   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   7    |   20    |   0.607038   |     -      |     -     |   1.30   \n","   7    |   40    |   0.613281   |     -      |     -     |   1.22   \n","   7    |   60    |   0.602789   |     -      |     -     |   1.22   \n","   7    |   80    |   0.608904   |     -      |     -     |   1.23   \n","   7    |   95    |   0.588942   |     -      |     -     |   0.89   \n","----------------------------------------------------------------------\n","Eval 7  |    -    |   0.605015   |  0.587402  |   70.74   |   6.53   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   8    |   20    |   0.582659   |     -      |     -     |   1.30   \n","   8    |   40    |   0.599857   |     -      |     -     |   1.22   \n","   8    |   60    |   0.608377   |     -      |     -     |   1.22   \n","   8    |   80    |   0.593297   |     -      |     -     |   1.22   \n","   8    |   95    |   0.578345   |     -      |     -     |   0.89   \n","----------------------------------------------------------------------\n","Eval 8  |    -    |   0.593142   |  0.578310  |   70.23   |   6.51   \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n","time consuming : 53.53s\n"]}],"source":["\n","import time\n","start = time.process_time()\n","set_seed(42)    # Set seed for reproducibility\n","\n","bert_classifier = DistilBertClassifier()\n","bert_classifier.to(device)\n","\n","# Create the optimizer\n","optimizer = torch.optim.AdamW(bert_classifier.parameters(),\n","                  lr=5e-5,    # Default learning rate\n","                  eps=1e-8    # Default epsilon value\n","                  )\n","\n","mytrainer(bert_classifier, optimizer, train_dataloader, val_dataloader, epochs=8, evaluation=True)\n","end = time.process_time()\n","print(\"time consuming : {:.2f}s\".format(end - start))\n"]},{"cell_type":"markdown","metadata":{"id":"H47ITYNLwJhb"},"source":["We will now fine-tune the DistilBert model by unfreezing specific layers in the pre-trained model, and training our full model (encoder + classifier) again."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"1Bv-hVz2wVyL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652807293418,"user_tz":-120,"elapsed":66763,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"e55f3c85-3659-45f8-b7c0-9bc99da13e55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   1    |   20    |   0.710361   |     -      |     -     |   1.66   \n","   1    |   40    |   0.620784   |     -      |     -     |   1.59   \n","   1    |   60    |   0.585653   |     -      |     -     |   1.59   \n","   1    |   80    |   0.568435   |     -      |     -     |   1.63   \n","   1    |   95    |   0.526863   |     -      |     -     |   1.21   \n","----------------------------------------------------------------------\n","Eval 1  |    -    |   0.607479   |  0.490266  |   73.98   |   8.39   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   0.442806   |     -      |     -     |   1.71   \n","   2    |   40    |   0.543812   |     -      |     -     |   1.65   \n","   2    |   60    |   0.491453   |     -      |     -     |   1.68   \n","   2    |   80    |   0.484708   |     -      |     -     |   1.68   \n","   2    |   95    |   0.503129   |     -      |     -     |   1.24   \n","----------------------------------------------------------------------\n","Eval 2  |    -    |   0.492139   |  0.509632  |   73.30   |   8.69   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   3    |   20    |   0.442514   |     -      |     -     |   1.75   \n","   3    |   40    |   0.444086   |     -      |     -     |   1.65   \n","   3    |   60    |   0.413776   |     -      |     -     |   1.63   \n","   3    |   80    |   0.445770   |     -      |     -     |   1.63   \n","   3    |   95    |   0.445899   |     -      |     -     |   1.19   \n","----------------------------------------------------------------------\n","Eval 3  |    -    |   0.438062   |  0.482373  |   79.03   |   8.54   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   4    |   20    |   0.382690   |     -      |     -     |   1.68   \n","   4    |   40    |   0.422226   |     -      |     -     |   1.61   \n","   4    |   60    |   0.416776   |     -      |     -     |   1.59   \n","   4    |   80    |   0.402281   |     -      |     -     |   1.58   \n","   4    |   95    |   0.378007   |     -      |     -     |   1.16   \n","----------------------------------------------------------------------\n","Eval 4  |    -    |   0.401378   |  0.529954  |   76.48   |   8.29   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   5    |   20    |   0.322614   |     -      |     -     |   1.62   \n","   5    |   40    |   0.372124   |     -      |     -     |   1.56   \n","   5    |   60    |   0.344005   |     -      |     -     |   1.55   \n","   5    |   80    |   0.379329   |     -      |     -     |   1.54   \n","   5    |   95    |   0.399673   |     -      |     -     |   1.13   \n","----------------------------------------------------------------------\n","Eval 5  |    -    |   0.361241   |  0.593755  |   75.68   |   8.05   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   6    |   20    |   0.324599   |     -      |     -     |   1.59   \n","   6    |   40    |   0.321699   |     -      |     -     |   1.52   \n","   6    |   60    |   0.352570   |     -      |     -     |   1.52   \n","   6    |   80    |   0.406049   |     -      |     -     |   1.51   \n","   6    |   95    |   0.357207   |     -      |     -     |   1.11   \n","----------------------------------------------------------------------\n","Eval 6  |    -    |   0.351886   |  0.549293  |   77.27   |   7.90   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   7    |   20    |   0.339193   |     -      |     -     |   1.57   \n","   7    |   40    |   0.310613   |     -      |     -     |   1.50   \n","   7    |   60    |   0.358917   |     -      |     -     |   1.50   \n","   7    |   80    |   0.337505   |     -      |     -     |   1.49   \n","   7    |   95    |   0.328918   |     -      |     -     |   1.10   \n","----------------------------------------------------------------------\n","Eval 7  |    -    |   0.335391   |  0.544861  |   77.33   |   7.79   \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   8    |   20    |   0.244999   |     -      |     -     |   1.56   \n","   8    |   40    |   0.290216   |     -      |     -     |   1.49   \n","   8    |   60    |   0.314272   |     -      |     -     |   1.50   \n","   8    |   80    |   0.305820   |     -      |     -     |   1.49   \n","   8    |   95    |   0.333944   |     -      |     -     |   1.10   \n","----------------------------------------------------------------------\n","Eval 8  |    -    |   0.295420   |  0.606154  |   75.74   |   7.77   \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n","time consuming : 66.74s\n"]}],"source":["import time\n","start = time.process_time()\n","set_seed(42)    # Set seed for reproducibility\n","\n","unfreeze_layers = [5]   # We'll fine-tune the last layer (update the weights of that specific layer) of distilBERT\n","\n","unfrozen_classifier = DistilBertClassifier(unfreeze_layers=unfreeze_layers)\n","\n","unfrozen_classifier.to(device)\n","\n","# Create the optimizer\n","optimizer = torch.optim.AdamW(unfrozen_classifier.parameters() )\n","mytrainer(unfrozen_classifier, optimizer, train_dataloader, val_dataloader, epochs=8, evaluation=True)\n","end = time.process_time()\n","print(\"time consuming : {:.2f}s\".format(end - start))"]},{"cell_type":"markdown","metadata":{"id":"AIfxBeMSw98Q"},"source":["### Question-3\n","\n","- Which classifier (`bert_classifier` or `unfrozen_classifier`) takes more time to train? Which one performs better in terms of validation accuracy? Explain. \n","\n","**<font color=blue><<< Unfrozen_classifier takes more time to train, as part of parameters have been unfrozen and updating more parameters will take more time.  It is still unfrozen_classifier that performs better in terms of validation accuracy, because the training and validation network configurations are different, and \"frozen\" may amplify the side effect in terms of accuracy. >>></font>**"]},{"cell_type":"markdown","metadata":{"id":"ARkLEiZg3Hht"},"source":["\n","# **Section 3: Multi-lingual transformers: LaBSE** \n","\n","In this exercise, we'll use yet another pre-trained transformer, namely the `Language agnostic BERT sentence embedding` model [LabSe](https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html). This is a multilingual sentence embedding model that encodes text from different languages into a shared embedding space. This allows it to be applied to a range of downstream tasks, like text classification, clustering, and others, while also leveraging semantic information for language understanding.\n","\n","We will fine-tune the LabSe model for a sentiment classifiction task based on small sample of the [YELP](https://www.yelp.com/dataset) dataset in English. \n","\n","We will then evaluate our fine-tuned model on a test set in another language (DUTCH, FRENCH) and inspect how agnostic our model really is to the language change."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNd7ciTS4ZUW"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0zLj2d6An9V"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModel\n","\n","# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGfyHy8p75Nr"},"outputs":[],"source":["#Load your data into a dataframe\n","yelp = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/yelp_train.csv\", encoding='latin-1')\n","\n","X = yelp.text.values\n","y = yelp.label.values\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"0gf0Hf_Cihjz"},"source":["As in the previous exercise, we will first create a preprocessing function that tokenizes the input text and tensorizes it. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMvzKUuF8NMi","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["47e6c39f39e245dc8fe5e6c8607cb702","e0df3c99723f45e3abf67b26e1149493","69bace5f185b476e8a97680450d49eca","f05046772cb84ede8a1426adc8626f40","f45b8fb62891432a9322d84c88398703","20d12ca5b44b4e2c9e53c4add9af2652","1da5f1b576c44e12bda8f48f27ea5855","0e5c3ebc42ab4ba38ea6a9249f84d3c6","ca5b438ff8a045eba7507679444e9bad","620e65a6992f4d29ad89f381ff969797","85a6d1929a9643c0bc0e78862091057a","66559b361d9444c5af805aef94ef776f","b28b5c07575c4acd8f086074ffc1d257","3d8a75f086374b608d43a6732b173c7e","24b954c9b3cc48e8b4506de2e2add7e9","0ee735b28f06446b8a1a2ff44545fa1b","42326dd875794990b5c2740e5f641a75","1d98dd4e4060492dafb0e4ef105c3243","d6e01088d4e548b78ce2c6677edca45b","b0303e15f6d94c7fa484e9b8d86a9ec5","8680d6c43a044e868ed58b039ac3be55","a8fd2e6aafd3484ea6a23f24ce6515c1","7e01c0ff92754aa3b0376150d1cea892","f982cdc6b2a74c3dbe67e3dc6c42c1a8","351ca8cb4acd49d6a807da870c846603","c4d525ee228d4b19972fe7d352d54cef","e49eeb05aa8c4887bed5409d3b84dab7","48020f4007b1424291e9d9e8a21e8723","d955db1cefa04c02a77c631af8f63597","cd96e633de604552b88915de8dcd0bc2","896e70e4b173438a82fd1c3abf21ba2a","5deb13d3557143f7a0f0068283264456","f4c8aeb98e594771925099b8421f0376","41396fdee18645c2847ebd4fc4bd9929","11b25cfe9847400080c3f446e6e219c7","fc6ffd25119143728446266e19524ed0","f9ed30cf036f4aeca7628ce556b4a071","aa3c40d4d7074d11b0c5a228727742c1","dcafa8fc603945abb23a6ec8921ede06","69782995979a49ad953dbd029612b8c9","305af9bf75da49998e91f170e07dbc4b","8b8c2a051a234d7eb189fd658fae311a","ee0477a15a8c40f89dd1a6fb59fb741f","31e04a0c3cd4468ca7513040b1514e20"]},"executionInfo":{"status":"ok","timestamp":1652803475779,"user_tz":-120,"elapsed":5982,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"e65ec3fd-189d-48af-b166-de716a5d3422"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/472 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e6c39f39e245dc8fe5e6c8607cb702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66559b361d9444c5af805aef94ef776f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e01c0ff92754aa3b0376150d1cea892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41396fdee18645c2847ebd4fc4bd9929"}},"metadata":{}}],"source":["MAX_LEN = 64\n","mytokenizer = AutoTokenizer.from_pretrained(\"pvl/labse_bert\", do_lower_case=False)\n","\n","# Create a function to tokenize a set of texts\n","def preprocessing_for_labse(data):\n","    input_ids = []\n","    attention_masks = []\n","    for sent in data:\n","\n","        encoded_sent = mytokenizer(\n","            text=sent,           \n","            add_special_tokens=True,               \n","            max_length=MAX_LEN,                    \n","            padding='max_length',                        \n","            truncation =True,                       \n","            return_attention_mask = True\n","            )\n","\n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","    \n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7uCx_lU85Nv"},"outputs":[],"source":["train_inputs, train_masks = preprocessing_for_labse(X_train)\n","val_inputs, val_masks = preprocessing_for_labse(X_val)"]},{"cell_type":"markdown","metadata":{"id":"V9ql0Xov9AWC"},"source":["Again, we create dataloader which allows us to easily extract batches for training and validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pejzGEKn86cd"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train)\n","val_labels = torch.tensor(y_val)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"]},{"cell_type":"markdown","metadata":{"id":"2s8bX8jW9jPv"},"source":["Now, we define the LabSeClassifier model architecture. Once again, we initially opt to freeze all parameters in the pre-trained model. Please complete the code where indicated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Qif1boT7VDA"},"outputs":[],"source":["class LabSeClassifier(torch.nn.Module):\n","    def __init__(self):\n","        super(LabSeClassifier, self).__init__()\n","        H, D_out = 768, 2 \n","\n","        self.labse_encoder = AutoModel.from_pretrained(\"pvl/labse_bert\")\n","         \n","        #Freeze all the parameters of the LabSe encoder\n","\n","        ############### for student ################ \n","        for param in self.labse_encoder.parameters():\n","            param.requires_grad = False\n","\n","        ############################################\n","\n","        self.linear = torch.nn.Linear(H, D_out)       \n","\n","    def forward(self, input_ids, attention_mask):\n","\n","        output = self.labse_encoder(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        ### Mean pool over the hidden representations of each token \n","        ### to get a single vector representation for the whole sentence \n","\n","        token_embeddings = output[0]                                            \n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        labse_representation = sum_embeddings/sum_mask\n","         \n","        logits = self.linear(labse_representation)\n","        return logits\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0XjN28Pg9xe9"},"source":["Since we only defined another model and don't aim to change anything about the training strategy, we can reuse the `mytrainer()` function from section 2 (initially used to train the DistilBERT-based tweet classifier). Below, we train the LabSeClassifier. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"on6oljnP9RqV"},"outputs":[],"source":["import random\n","import numpy as np\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","set_seed(42)    # Set seed for reproducibility\n","\n","\n","labse_classifier = LabSeClassifier()\n","\n","labse_classifier.to(device)\n","\n","optimizer = torch.optim.AdamW(labse_classifier.parameters(),\n","                  lr=5e-5,    # Default learning rate\n","                  eps=1e-8    # Default epsilon value\n","                  )\n","\n","mytrainer(labse_classifier, optimizer, train_dataloader, val_dataloader, epochs=5, evaluation=True)      \n"]},{"cell_type":"markdown","metadata":{"id":"JAkv7tnJ-Szq"},"source":["The `labse_predict` function below takes the trained model as an input, together with a test set of unseen instances, and predicts the sentiment of the examples in the test set. It does this by simply passing the inputs through the trained model, and transforming the obtained logits into a probability distribution of sentiment classes.\n","\n","We also define a function to determine the accuracy of the model's sentiment predictions over the test set. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjO3hTE7-j5o"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def labse_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained model to predict probabilities\n","    on the test set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","\n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs\n","\n","\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy0duOCV-vZ2"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def accuracy_function(yelp_test):\n"," \n","    test_x = yelp_test.text.values      \n","    test_y = yelp_test.label.values\n","  \n","\n","    test_labels = torch.tensor(test_y)\n","\n","    test_inputs, test_masks = preprocessing_for_labse(test_x)      #Preprocess the test instance \n","\n","    #Prepare testdataloader that will be used by the trained model\n","    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","  \n","\n","    #Predict the probabilites\n","    probs = labse_predict(labse_classifier, test_dataloader)\n","    preds = probs[:, 1]\n","\n","    y_pred = np.where(preds >= 0.5, 1, 0)     # if probability prediction is >=0.5 then it's class 1, and 0 otherwise \n","\n","    accuracy = accuracy_score(test_labels, y_pred)\n","    return accuracy \n","  \n","     "]},{"cell_type":"markdown","metadata":{"id":"sQbhZp1q-YwW"},"source":["Now that we have trained a LabSe classifier, which is fine-tuned on English sentences for the task of sentiment prediction, we can evaluate it. Not only will we evaluate its performance on an English test set, we will also test out several other languages (Dutch, French and Italian)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywl7xoMMAMr-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652803906266,"user_tz":-120,"elapsed":3373,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"8d7c1849-7caa-4360-c730-6421da051a2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy English: 75.50%\n","Accuracy Dutch: 78.00%\n","Accuracy French: 72.50%\n","Accuracy Italian: 77.00%\n"]}],"source":["\n","yelp_test_nl = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/nyelp_test_nl.csv\", encoding='latin-1')\n","yelp_test_fr = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/nyelp_test_fr.csv\", encoding='latin-1')\n","yelp_test_en = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/yelp_test.csv\", encoding='latin-1')\n","yelp_test_it = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/nyelp_test_it.csv\", encoding='latin-1')\n","\n","nl_accuracy = accuracy_function(yelp_test_nl)\n","fr_accuracy = accuracy_function(yelp_test_fr)\n","en_accuracy = accuracy_function(yelp_test_en)\n","it_accuracy = accuracy_function(yelp_test_it)\n","\n","print(f'Accuracy English: {en_accuracy*100:.2f}%')\n","print(f'Accuracy Dutch: {nl_accuracy*100:.2f}%')\n","print(f'Accuracy French: {fr_accuracy*100:.2f}%')\n","print(f'Accuracy Italian: {it_accuracy*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"AvscZMK_AS4R"},"source":["### Question-4\n","\n","Why would the model work on other languages than the one it was fine-tuned for (English)? Describe the pre-training strategy that leads to this 'language-agnostic' property of the LabSe model. You can consult the linked [blog post from the creators of the LabSe model](https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html) and other online sources to solve this question (please list the ones you used). \n","\n","**<font color=blue><<< INSERT ANSWER HERE >>></font>** "]},{"cell_type":"markdown","metadata":{"id":"tpZs_939HsO3"},"source":["# **Section 4: Fine-tuning T5 For Seq-to-Seq Task**"]},{"cell_type":"markdown","metadata":{"id":"4_KGxr0PClDE"},"source":["So far, we have focused only on *classifiers* built on the top of Transformers. In this last exercise, we'll use a pretrained *sequence-to-sequence* transformer model to generate a summary for a given news article. Instead of outputting a probability distribution over classes (as is the case for classification), this model will take a text as an input, and output another text (hence, sequence-to-sequence). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtz2F3wFH2A_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652803921500,"user_tz":-120,"elapsed":11575,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"87c430d6-fd6a-4c6b-c717-bd78311818b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (3.0.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["!pip install Sentencepiece\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otAGkM0qH2FZ"},"outputs":[],"source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wcZQe0uH2IO"},"outputs":[],"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rab6ghAcH-yt","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1652804058218,"user_tz":-120,"elapsed":972,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"18f9fa39-e0e7-4b0a-de6b-65c66141dfc3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  \\\n","0  The Administration of Union Territory Daman an...   \n","1  Malaika Arora slammed an Instagram user who tr...   \n","2  The Indira Gandhi Institute of Medical Science...   \n","3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n","4  Hotels in Maharashtra will train their staff t...   \n","\n","                                               ctext  \n","0  The Daman and Diu administration on Wednesday ...  \n","1  From her special numbers to TV?appearances, Bo...  \n","2  The Indira Gandhi Institute of Medical Science...  \n","3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n","4  Hotels in Mumbai and other Indian cities are t...  "],"text/html":["\n","  <div id=\"df-32837125-f523-4b5c-9e76-32c6d65e1871\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>ctext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Administration of Union Territory Daman an...</td>\n","      <td>The Daman and Diu administration on Wednesday ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Malaika Arora slammed an Instagram user who tr...</td>\n","      <td>From her special numbers to TV?appearances, Bo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","      <td>The Indira Gandhi Institute of Medical Science...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hotels in Maharashtra will train their staff t...</td>\n","      <td>Hotels in Mumbai and other Indian cities are t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32837125-f523-4b5c-9e76-32c6d65e1871')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-32837125-f523-4b5c-9e76-32c6d65e1871 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-32837125-f523-4b5c-9e76-32c6d65e1871');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":67}],"source":["df = pd.read_csv(\"https://raw.githubusercontent.com/semerekiros/ML_NLP/main/news_summary_small.csv\", encoding='latin-1')\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"Dl9pynN1IQre"},"source":["In the previous two exercises, we wrote our own preprocessing functions before creating our dataloaders. Here we'll directly use the `Dataset` module provided by `PyTorch`. It defines both how text is pre-processed and stores the instances with their corresponding labels. A `Dataloader` method then wraps an iterable around the `Dataset` in order to enable easy access to the samples before sending them to the neural network. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LyHUINbH-1W"},"outputs":[],"source":["class SummaryDataset(Dataset):\n","\n","    def __init__(self, df, tokenizer, source_len, summary_len):\n","        self.tokenizer = tokenizer\n","        self.data = df\n","        self.source_len = source_len\n","        self.summary_len = summary_len\n","        self.summarys = self.data.text\n","        self.articles = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.summarys)\n","\n","    def __getitem__(self, index):\n","        article = str(self.articles[index])\n","        article = ' '.join(article.split())\n","\n","        summary = str(self.summarys[index])\n","        summary = ' '.join(summary.split())\n","\n","        source = self.tokenizer.batch_encode_plus([article], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([summary], max_length= self.summary_len, pad_to_max_length=True,return_tensors='pt')\n","\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask']\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"EmsLK1DEJdvz"},"source":["As opposed to the previous exercises, we won't add any layers on top of the pre-trained transformer. This is because the pre-trained transformer we will use is already a sequence-to-sequence model, as is the task of providing a summary for a news article. \n","\n","We now immediately advance to defining the training loop. Note that the model's forward function (which we will load in later) takes the following arguments as an input: \n","- the input sequence IDs\n","- the attention mask\n","- the decoder input IDs\n","- the labels \n","\n","For more information about these arguments, please refer to [T5Model](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFiQFmq_JcfS"},"outputs":[],"source":["def train(epoch, tokenizer, model, device, loader, optimizer):\n","\n","    #Freeze the encoder part of the t5, to limit required computational resources\n","    for par in model.get_encoder().parameters():    \n","        par.requires_grad = False\n","  \n","    model.train()\n","    for _, data in enumerate(loader,0):\n","        y = data['target_ids'].to(device, dtype=torch.long)\n","        y_ids = y[:,:-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels [y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype=torch.long)\n","        mask = data['source_mask'].to(device, dtype=torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","\n","        if _%10 == 0:\n","            print(f'Training Loss: {loss.item()}')\n","        if _%500==0:\n","            print(f'Epoch:{epoch}, Loss: {loss.item()}')\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n"]},{"cell_type":"markdown","metadata":{"id":"Gnh0OjE7FHkB"},"source":["The following function validates the performance of the model. It asks the model to generate an output sequence, based on a given input sequence. In our case, the input sequence will be a news article, and the output sequence will be its summary. To evaluate the model, we will calculate the BLEU-score between the predicted summary and the target summary. We will do this for all the news articles in our summary validation set, while training on the train set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZkKlP04Vy6t"},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype=torch.long)\n","            ids = data['source_ids'].to(device, dtype =torch.long)\n","            mask = data['source_mask'].to(device, dtype =torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                max_length = 150,\n","                num_beams=2,\n","                repetition_penalty = 2.5,\n","                length_penalty=1.0,\n","                early_stopping=True\n","            )\n","\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","\n","            if _%10 == 0:\n","                print(f'Completed {_}')\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","\n","    return predictions, actuals\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8BKI3PJFZk1"},"outputs":[],"source":["from torchtext.data.metrics import bleu_score\n","\n","def calculate_bleu(predictions, actuals):\n","    predictions = [prediction.split(\" \") for prediction in predictions]\n","    actuals = [[actual.split(\" \")] for actual in actuals]\n","\n","    score = bleu_score(predictions, actuals)\n","\n","\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"NBR6RzKeKDUD"},"source":["Now, we load in the `Dataset` class as defined before and prepare the train and validation `Dataloader`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhe95gTbKHCz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652804080196,"user_tz":-120,"elapsed":1374,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"f4edaaff-d875-45fa-b280-2e15a53e137b"},"outputs":[{"output_type":"stream","name":"stdout","text":["FULL Dataset: (1000, 2)\n","TRAIN Dataset: (800, 2)\n","VAL Dataset: (200, 2)\n"]}],"source":["model_config={\n","    \"MODEL\":\"t5-base\",             # model_type: t5-base/t5-large\n","    \"TRAIN_BATCH_SIZE\":8,          # training batch size\n","    \"VALID_BATCH_SIZE\":20,          # validation batch size\n","    \"TRAIN_EPOCHS\":2,              # number of training epochs\n","    \"VAL_EPOCHS\":1,                # number of validation epochs\n","    \"LEARNING_RATE\":1e-4,          # learning rate\n","    \"MAX_LEN\":512,  # max length of source text\n","    \"SUMMARY_LEN\":150,   # max length of target text\n","    \"SEED\": 42                     # set seed for reproducibility \n","}\n","\n","\n","# Set random seeds and deterministic pytorch for reproducibility\n","torch.manual_seed(model_config[\"SEED\"]) # pytorch random seed\n","np.random.seed(model_config[\"SEED\"]) # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","# tokenzier for encoding the text\n","mytokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","\n","\n","df.ctext = 'summarize: ' + df.ctext   #pre-append each text with 'summarize' prompt\n","\n","\n","# Creation of Dataset and Dataloader\n","# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n","train_size = 0.8\n","train_dataset=df.sample(frac=train_size,random_state = model_config[\"SEED\"])\n","val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"VAL Dataset: {}\".format(val_dataset.shape))\n","\n","\n","\n","# Creating the Training and Validation dataset for further creation of Dataloader\n","training_set = SummaryDataset(train_dataset, mytokenizer, model_config[\"MAX_LEN\"], model_config[\"SUMMARY_LEN\"])\n","val_set = SummaryDataset(val_dataset, mytokenizer, model_config[\"MAX_LEN\"], model_config[\"SUMMARY_LEN\"])\n","\n","# Defining the parameters for creation of dataloaders\n","train_params = {\n","    'batch_size': model_config[\"TRAIN_BATCH_SIZE\"],\n","    'shuffle': True,\n","    'num_workers': 0\n","    }\n","\n","val_params = {\n","    'batch_size': model_config[\"VALID_BATCH_SIZE\"],\n","    'shuffle': False,\n","    'num_workers': 0\n","    }\n","\n","# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","training_loader = DataLoader(training_set, **train_params)\n","val_loader = DataLoader(val_set, **val_params)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yLK2YntPPK2F"},"source":["We now take the pre-trained T5 language generation model, and finetune it on our summary dataset to create the `T5 Summarizer`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKctdxE1ODtG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652804186980,"user_tz":-120,"elapsed":101022,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"cbad0257-994b-404e-a351-ea420a78220e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Initiating Fine-Tuning for the model on our dataset\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 6.882297039031982\n","Epoch:0, Loss: 6.882297039031982\n","Training Loss: 2.7232818603515625\n","Training Loss: 2.2250008583068848\n","Training Loss: 2.7008156776428223\n","Training Loss: 1.978935956954956\n","Training Loss: 1.710108995437622\n","Training Loss: 2.1695332527160645\n","Training Loss: 1.5600254535675049\n","Training Loss: 1.7268074750900269\n","Training Loss: 1.924350380897522\n","Training Loss: 1.8997857570648193\n","Epoch:1, Loss: 1.8997857570648193\n","Training Loss: 2.055452346801758\n","Training Loss: 1.7036826610565186\n","Training Loss: 1.5796823501586914\n","Training Loss: 2.298892021179199\n","Training Loss: 1.6873408555984497\n","Training Loss: 2.1409990787506104\n","Training Loss: 1.4904634952545166\n","Training Loss: 1.5885956287384033\n","Training Loss: 1.582797646522522\n"]}],"source":["# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n","# Further this model is sent to device (GPU/TPU) for using the hardware.\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","model = model.to(device)\n","\n","# Defining the optimizer that will be used to tune the weights of the network in the training session. \n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_config[\"LEARNING_RATE\"])\n","\n","# Training loop\n","print('Initiating Fine-Tuning for the model on our dataset')\n","\n","for epoch in range(model_config[\"TRAIN_EPOCHS\"]):\n","    train(epoch, mytokenizer, model, device, training_loader, optimizer)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vrp1c8tuHAHO"},"source":["Now, let's evaluate the performance of our summarizer in terms of the BLEU score metric. We can use the `validate` and `calculate_bleu` functions we defined earlier. You can ignore the warnings below. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RifGtOqnbkNB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652804252026,"user_tz":-120,"elapsed":56620,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"f546844f-4051-4c6f-a66b-fb71bb30db4e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1777: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  next_indices = next_tokens // vocab_size\n"]},{"output_type":"stream","name":"stdout","text":["Completed 0\n","0.11010953100057075\n"]}],"source":["predictions, actuals = validate(1, mytokenizer, model, device, val_loader)   #Use the model to get predictions\n","score = calculate_bleu(predictions, actuals)           #Calculate the bleu score between the predictions and the ground truth summaries\n","print(score)"]},{"cell_type":"markdown","metadata":{"id":"T-CLc-gjeyVr"},"source":["# Let's test it!\n","\n","We will now have a look at the news article summaries our model comes up with. We take the first instance in our validation set and ask our model to generate a summary. To do this, we'll use the `generate` function. As an input, we provide the same prompt as was used during finetuning of the model (`summarize: <news article>`). Thanks to the training process, the language generation model will know how to generate a summary of the article. \n","\n","If you are looking for a good read on the underlying techniques for text generation (greedy search, beam search, ...) and some examples of how to use the `generate` method, please have a look at this [blog post from huggingface](https://huggingface.co/blog/how-to-generate)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eytHRK1HansD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652804258660,"user_tz":-120,"elapsed":384,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"97a1cfdb-d36c-4e01-cf29-41fda7e94dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["summarize: Hotels in Mumbai and other Indian cities are to train their staff to spot signs of sex trafficking such as frequent requests for bed linen changes or a \"Do not disturb\" sign left on the door for days on end. The group behind the initiative is also developing a mobile phone app - Rescue Me - which hotel staff can use to alert local police and senior anti-trafficking officers if they see suspicious behavior. \"Hotels are breeding grounds for human trade,\" said Sanee Awsarmmel, chairman of the alumni group of Maharashtra State Institute of Hotel Management and Catering Technology. \"(We) have hospitality professionals working in hotels across the country. We are committed to this cause.\"The initiative, spearheaded by the alumni group and backed by the Maharashtra state government, comes amid growing international recognition that hotels have a key role to play in fighting modern day slavery. MAHARASHTRA MAJOR DESTINATION FOR TRAFFICKED GIRLS Maharashtra, of which Mumbai is the capital, is a major destination for trafficked girls who are lured from poor states and nearby countries on the promise of jobs, but then sold into the sex trade or domestic servitude. With rising property prices, some traditional red light districts like those in Mumbai have started to disappear pushing the sex trade underground into private lodges and hotels, which makes it hard for police to monitor.Awsarmmel said hotels would be told about 50 signs that staff needed to watch out for.These include requests for rooms with a view of the car park which are favored by traffickers as they allow them to vet clients for signs of trouble and check out their cars to gauge how much to charge.Awsarmmel said hotel staff often noticed strange behavior such as a girl's reticence during the check-in process or her dependence on the person accompanying her to answer questions and provide her proof of identity.But in most cases, staff ignore these signs or have no idea what to do, he told the Thomson Reuters Foundation.RESCUE ME APP The Rescue Me app - to be launched in a couple of months - will have a text feature where hotel staff can fill in details including room numbers to send an alert to police.Human trafficking is the world's fastest growing criminal enterprise worth an estimated $150 billion a year, according to the International Labor Organization, which says nearly 21 million people globally are victims of forced labor and trafficking.Last year, major hotel groups, including the Hilton and Shiva Hotels, pledged to examine their supply chains for forced labor, and train staff how to spot and report signs of trafficking.Earlier this year, Mexico City also launched an initiative to train hotel staff about trafficking.Vijaya Rahatkar, chairwoman of the Maharashtra State Women's Commission, said the initiative would have an impact beyond the state as the alumni group had contact with about a million small hotels across India.The group is also developing a training module on trafficking for hotel staff and hospitality students which could be used across the country.ALSO READFYI | Legal revenge: Child sex trafficking survivors get 'School of Justice' to fight their own battlesMumbai: Woman DJ arrested in high-profile sex racket case\n"]}],"source":["test_sent = val_dataset.ctext.values[1]\n","print(test_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b8V62gdWw8B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652804266029,"user_tz":-120,"elapsed":1467,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"8dca46df-96ca-4928-c22c-90d9671abd30"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Decoding strategy: Beam search, \n"," Generated summary:  hotels in Mumbai and other Indian cities are to train staff to spot signs of sex trafficking such as frequent requests for bed linen changes or a \"Do not disturb\" sign left on the door for days on end. The initiative is also developing a mobile phone app which staff can use to alert local police and senior anti-trafficking officers if they see suspicious behavior.\n"]}],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","test_tokenized = mytokenizer.encode_plus(test_sent, max_length = model_config[\"MAX_LEN\"], pad_to_max_length=True, return_tensors=\"pt\")\n","\n","test_input_ids  = test_tokenized[\"input_ids\"]\n","test_attention_mask = test_tokenized[\"attention_mask\"]\n","\n","test_input_ids = test_input_ids.to(device, dtype=torch.long)\n","test_attention_mask = test_attention_mask.to(device, dtype=torch.long)\n","\n","model.eval()\n","\n","beam_outputs = model.generate(\n","    input_ids = test_input_ids,\n","    attention_mask = test_attention_mask,\n","    max_length = 150,\n","    num_beams=2,\n","    repetition_penalty = 2.5,\n","    length_penalty=1.0,\n","    early_stopping=True\n",")\n","\n","for beam_output in beam_outputs:\n","    predicted_summary = mytokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","    print(f' Decoding strategy: Beam search, \\n Generated summary:  {predicted_summary}')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZS0IigLaZgt5"},"source":["Beam search was used to generate the previous summary. As is mentioned in the [blog post](https://huggingface.co/blog/how-to-generate) many other decoding strategies can be used to generate output sequence, given an input sequence. Here, we ask you to use nucleus sampling, also called `top-p sampling`. Have a look at how we implemented the beam-search decoding strategy in the code above (`beam_outputs = model.generate(...)`), and add some arguments which ensure that top-p sampling is used instead of beam search."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKJtqd1PZeJY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652805466824,"user_tz":-120,"elapsed":2265,"user":{"displayName":"Louise Fan Chou","userId":"02530256663998508643"}},"outputId":"f1636bb9-4007-4019-8785-bab58eb19dea"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Decoding strategy: Nucleus sampling, \n"," Generated summary:  hotels in Mumbai and other Indian cities are to train staff to spot signs of sex trafficking such as frequent requests for bed linen changes or a \"Do not disturb\" sign left on the door for days on end. The initiative is also developing a mobile phone app which staff can use to alert local police and senior anti-trafficking officers if they see suspicious behavior.\n"]}],"source":["# Write the decoding strategy with nucleus sampling \n","#  \n","# sample_outputs = model.generate (\n","#    ...\n","#    specify some additional arguments to implement top-p sampling with a probability of 0.88\n","#  )\n","\n","sample_outputs = model.generate(\n","    input_ids = test_input_ids,\n","    attention_mask = test_attention_mask,\n","    max_length = 150,\n","    num_beams=2,\n","    repetition_penalty = 2.5,\n","    length_penalty=1.0,\n","    early_stopping=True,\n","    ############### for student ###################\n","    top_p=0.88,\n","    ##############################################\n",")\n","\n","\n","for sample_output in sample_outputs:\n","    predicted_summary =  mytokenizer.decode(sample_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","    print(f' Decoding strategy: Nucleus sampling, \\n Generated summary:  {predicted_summary}')"]},{"cell_type":"markdown","metadata":{"id":"u92QVSRRo_gQ"},"source":["### Acknowledgment\n","\n","If you received help or feedback from fellow students, please acknowledge that here. We count on your academic honesty:\n","\n","**<font color=blue><<< LIST POTENTIAL COLLABORATORS HERE >>></font>**\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NLP_lab4_student.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"47e6c39f39e245dc8fe5e6c8607cb702":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0df3c99723f45e3abf67b26e1149493","IPY_MODEL_69bace5f185b476e8a97680450d49eca","IPY_MODEL_f05046772cb84ede8a1426adc8626f40"],"layout":"IPY_MODEL_f45b8fb62891432a9322d84c88398703"}},"e0df3c99723f45e3abf67b26e1149493":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20d12ca5b44b4e2c9e53c4add9af2652","placeholder":"​","style":"IPY_MODEL_1da5f1b576c44e12bda8f48f27ea5855","value":"Downloading: 100%"}},"69bace5f185b476e8a97680450d49eca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e5c3ebc42ab4ba38ea6a9249f84d3c6","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca5b438ff8a045eba7507679444e9bad","value":472}},"f05046772cb84ede8a1426adc8626f40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_620e65a6992f4d29ad89f381ff969797","placeholder":"​","style":"IPY_MODEL_85a6d1929a9643c0bc0e78862091057a","value":" 472/472 [00:00&lt;00:00, 4.36kB/s]"}},"f45b8fb62891432a9322d84c88398703":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20d12ca5b44b4e2c9e53c4add9af2652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1da5f1b576c44e12bda8f48f27ea5855":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e5c3ebc42ab4ba38ea6a9249f84d3c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca5b438ff8a045eba7507679444e9bad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"620e65a6992f4d29ad89f381ff969797":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85a6d1929a9643c0bc0e78862091057a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66559b361d9444c5af805aef94ef776f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b28b5c07575c4acd8f086074ffc1d257","IPY_MODEL_3d8a75f086374b608d43a6732b173c7e","IPY_MODEL_24b954c9b3cc48e8b4506de2e2add7e9"],"layout":"IPY_MODEL_0ee735b28f06446b8a1a2ff44545fa1b"}},"b28b5c07575c4acd8f086074ffc1d257":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42326dd875794990b5c2740e5f641a75","placeholder":"​","style":"IPY_MODEL_1d98dd4e4060492dafb0e4ef105c3243","value":"Downloading: 100%"}},"3d8a75f086374b608d43a6732b173c7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6e01088d4e548b78ce2c6677edca45b","max":5220781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0303e15f6d94c7fa484e9b8d86a9ec5","value":5220781}},"24b954c9b3cc48e8b4506de2e2add7e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8680d6c43a044e868ed58b039ac3be55","placeholder":"​","style":"IPY_MODEL_a8fd2e6aafd3484ea6a23f24ce6515c1","value":" 5.22M/5.22M [00:00&lt;00:00, 13.4MB/s]"}},"0ee735b28f06446b8a1a2ff44545fa1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42326dd875794990b5c2740e5f641a75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d98dd4e4060492dafb0e4ef105c3243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6e01088d4e548b78ce2c6677edca45b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0303e15f6d94c7fa484e9b8d86a9ec5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8680d6c43a044e868ed58b039ac3be55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8fd2e6aafd3484ea6a23f24ce6515c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e01c0ff92754aa3b0376150d1cea892":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f982cdc6b2a74c3dbe67e3dc6c42c1a8","IPY_MODEL_351ca8cb4acd49d6a807da870c846603","IPY_MODEL_c4d525ee228d4b19972fe7d352d54cef"],"layout":"IPY_MODEL_e49eeb05aa8c4887bed5409d3b84dab7"}},"f982cdc6b2a74c3dbe67e3dc6c42c1a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48020f4007b1424291e9d9e8a21e8723","placeholder":"​","style":"IPY_MODEL_d955db1cefa04c02a77c631af8f63597","value":"Downloading: 100%"}},"351ca8cb4acd49d6a807da870c846603":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd96e633de604552b88915de8dcd0bc2","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_896e70e4b173438a82fd1c3abf21ba2a","value":112}},"c4d525ee228d4b19972fe7d352d54cef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5deb13d3557143f7a0f0068283264456","placeholder":"​","style":"IPY_MODEL_f4c8aeb98e594771925099b8421f0376","value":" 112/112 [00:00&lt;00:00, 906B/s]"}},"e49eeb05aa8c4887bed5409d3b84dab7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48020f4007b1424291e9d9e8a21e8723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d955db1cefa04c02a77c631af8f63597":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd96e633de604552b88915de8dcd0bc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"896e70e4b173438a82fd1c3abf21ba2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5deb13d3557143f7a0f0068283264456":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4c8aeb98e594771925099b8421f0376":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41396fdee18645c2847ebd4fc4bd9929":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11b25cfe9847400080c3f446e6e219c7","IPY_MODEL_fc6ffd25119143728446266e19524ed0","IPY_MODEL_f9ed30cf036f4aeca7628ce556b4a071"],"layout":"IPY_MODEL_aa3c40d4d7074d11b0c5a228727742c1"}},"11b25cfe9847400080c3f446e6e219c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcafa8fc603945abb23a6ec8921ede06","placeholder":"​","style":"IPY_MODEL_69782995979a49ad953dbd029612b8c9","value":"Downloading: 100%"}},"fc6ffd25119143728446266e19524ed0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_305af9bf75da49998e91f170e07dbc4b","max":62,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b8c2a051a234d7eb189fd658fae311a","value":62}},"f9ed30cf036f4aeca7628ce556b4a071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee0477a15a8c40f89dd1a6fb59fb741f","placeholder":"​","style":"IPY_MODEL_31e04a0c3cd4468ca7513040b1514e20","value":" 62.0/62.0 [00:00&lt;00:00, 658B/s]"}},"aa3c40d4d7074d11b0c5a228727742c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcafa8fc603945abb23a6ec8921ede06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69782995979a49ad953dbd029612b8c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"305af9bf75da49998e91f170e07dbc4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b8c2a051a234d7eb189fd658fae311a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee0477a15a8c40f89dd1a6fb59fb741f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e04a0c3cd4468ca7513040b1514e20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ff0243ee93e4281975d82bd8d5e1f45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e81b69386904bf0b524cfde0694a47d","IPY_MODEL_0d1cb03c92074f20a063206567d301f4","IPY_MODEL_730bed6d91c7453ba0a9cab522c8f9ae"],"layout":"IPY_MODEL_bd3eca54ffbc4e6399fbbbc6bdc0c17e"}},"4e81b69386904bf0b524cfde0694a47d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74c027a3b84a4f7383fcab4c5c863a00","placeholder":"​","style":"IPY_MODEL_a99b6a70217f4dc99698f10d761f158c","value":"Downloading: 100%"}},"0d1cb03c92074f20a063206567d301f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f3e1bf4a10241e1a8accc8c58924281","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b56e2aa1577477eac7e8dfd15b998f9","value":791656}},"730bed6d91c7453ba0a9cab522c8f9ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1bd6c20dac14528b689e06a9658a801","placeholder":"​","style":"IPY_MODEL_79734ffc76554622bad1b4713b7d749b","value":" 792k/792k [00:00&lt;00:00, 1.56MB/s]"}},"bd3eca54ffbc4e6399fbbbc6bdc0c17e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c027a3b84a4f7383fcab4c5c863a00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a99b6a70217f4dc99698f10d761f158c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f3e1bf4a10241e1a8accc8c58924281":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b56e2aa1577477eac7e8dfd15b998f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1bd6c20dac14528b689e06a9658a801":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79734ffc76554622bad1b4713b7d749b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64722ef0fb81494a9e0d641d424c6c5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_daf3b2e4b5c64d379e5b1816fb19e74b","IPY_MODEL_4028821f2d814c6ca690694ec9ae3a8c","IPY_MODEL_d3f8a9af5a824677b01258daa403d99d"],"layout":"IPY_MODEL_1f0d142d69094a14bf4e0a3060310dd0"}},"daf3b2e4b5c64d379e5b1816fb19e74b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88d250bc7a7346eb80a2b842ff4e1120","placeholder":"​","style":"IPY_MODEL_a9ed962539034f8da10189802d1eb533","value":"Downloading: 100%"}},"4028821f2d814c6ca690694ec9ae3a8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56e97bc6d1ec40cd979cbfd5278c77bf","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_499bac713ed7441394562639f07d0486","value":1389353}},"d3f8a9af5a824677b01258daa403d99d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c2493b8fd254543aaec98cad3356b2c","placeholder":"​","style":"IPY_MODEL_f9192b497a894270844e66679d2cda3f","value":" 1.39M/1.39M [00:00&lt;00:00, 1.59MB/s]"}},"1f0d142d69094a14bf4e0a3060310dd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d250bc7a7346eb80a2b842ff4e1120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9ed962539034f8da10189802d1eb533":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56e97bc6d1ec40cd979cbfd5278c77bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"499bac713ed7441394562639f07d0486":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c2493b8fd254543aaec98cad3356b2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9192b497a894270844e66679d2cda3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcc9fbeac1774e5788449087996bb93b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_556d88337ebf47c6939f39fafc2c8b5f","IPY_MODEL_9735eb661a494868a3c65efa6f9ec592","IPY_MODEL_36debd85e189454689022c786eab150b"],"layout":"IPY_MODEL_2e5a2de569824df890b42f8586944561"}},"556d88337ebf47c6939f39fafc2c8b5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daea43cc82134905b155521451a6f73c","placeholder":"​","style":"IPY_MODEL_b4c5d6591d1e4cf5979089e0a53e6820","value":"Downloading: 100%"}},"9735eb661a494868a3c65efa6f9ec592":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c15f9021ecd64129816580b7935dc430","max":1199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51287e1de7474431a1281a86467d31f0","value":1199}},"36debd85e189454689022c786eab150b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b7caf06b34c49d1a24ed4fc018733d6","placeholder":"​","style":"IPY_MODEL_03aa010034c241688d806ed5428454ce","value":" 1.20k/1.20k [00:00&lt;00:00, 28.6kB/s]"}},"2e5a2de569824df890b42f8586944561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daea43cc82134905b155521451a6f73c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4c5d6591d1e4cf5979089e0a53e6820":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c15f9021ecd64129816580b7935dc430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51287e1de7474431a1281a86467d31f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b7caf06b34c49d1a24ed4fc018733d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03aa010034c241688d806ed5428454ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2cdb1623e394172b1f7b92215f780c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93381ac540bd4aa1acb9eaa76fb387fc","IPY_MODEL_bcb7c7ef800d45918e38b6c169c6e76b","IPY_MODEL_0804817deb024aa5bd4d9c331d1cbbb4"],"layout":"IPY_MODEL_fd58912a55cd48738797bfaff53ca88f"}},"93381ac540bd4aa1acb9eaa76fb387fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a36a45cd932c47aba445c14c923e20c4","placeholder":"​","style":"IPY_MODEL_c3a7cdc2f6164f5ca8d88fb68f2f4fc2","value":"Downloading: 100%"}},"bcb7c7ef800d45918e38b6c169c6e76b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83c16027a3a24c2ab10567ffe7700267","max":891691430,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c13d2cc0abe4848a815723207cc8407","value":891691430}},"0804817deb024aa5bd4d9c331d1cbbb4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0a0a52336864b4ea3bae90363dfbc45","placeholder":"​","style":"IPY_MODEL_af205d2db912485a922b1a4ce04f1b77","value":" 892M/892M [00:18&lt;00:00, 56.0MB/s]"}},"fd58912a55cd48738797bfaff53ca88f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a36a45cd932c47aba445c14c923e20c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a7cdc2f6164f5ca8d88fb68f2f4fc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83c16027a3a24c2ab10567ffe7700267":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c13d2cc0abe4848a815723207cc8407":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0a0a52336864b4ea3bae90363dfbc45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af205d2db912485a922b1a4ce04f1b77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c59038882bc54e34b27bedc89a6e4758":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1231241dce604afc833d50e9cf7342a7","IPY_MODEL_7f011f2613814adda6a64ea06063165f","IPY_MODEL_e571934030154e60ad64bbf6a2d3b02c"],"layout":"IPY_MODEL_1d44dfbcb06b4da28204fab7dea99d2b"}},"1231241dce604afc833d50e9cf7342a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b9e4ee1a38844c5bfe9df6badf7b16d","placeholder":"​","style":"IPY_MODEL_d29babd6b336433caae00485927caa77","value":"Downloading: 100%"}},"7f011f2613814adda6a64ea06063165f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4ae5baef7e44e19ca0a2c3639bfd78","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d9a682f0fb64fe58c6979ee5582326d","value":231508}},"e571934030154e60ad64bbf6a2d3b02c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f46c5800af1644e2b18447a51320fa67","placeholder":"​","style":"IPY_MODEL_47b220a627094340b0c1bcc66b03e0a2","value":" 232k/232k [00:00&lt;00:00, 2.46MB/s]"}},"1d44dfbcb06b4da28204fab7dea99d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b9e4ee1a38844c5bfe9df6badf7b16d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d29babd6b336433caae00485927caa77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e4ae5baef7e44e19ca0a2c3639bfd78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d9a682f0fb64fe58c6979ee5582326d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f46c5800af1644e2b18447a51320fa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47b220a627094340b0c1bcc66b03e0a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"604988535b8c49f6ab111c1496bc224f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8186aba6bf14030840a0418b11bad26","IPY_MODEL_9b1f151bccd64b658fd5a643c8a00c31","IPY_MODEL_479c7b25af1f4082bb9314d1464bc0a6"],"layout":"IPY_MODEL_27d16b49d22e4e92bfa03043850d52e4"}},"d8186aba6bf14030840a0418b11bad26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d603ac677adb487a93e6790de65aa00e","placeholder":"​","style":"IPY_MODEL_2adb709cf55443398770778797eb487d","value":"Downloading: 100%"}},"9b1f151bccd64b658fd5a643c8a00c31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5351bc2c2552476aade6b753037fd29d","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b1ea86698ff44178f6141d2496fa847","value":28}},"479c7b25af1f4082bb9314d1464bc0a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10322bbda984487d94461eed640b56e6","placeholder":"​","style":"IPY_MODEL_346beed676ea49e78776a32afa5fc5f7","value":" 28.0/28.0 [00:00&lt;00:00, 1.14kB/s]"}},"27d16b49d22e4e92bfa03043850d52e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d603ac677adb487a93e6790de65aa00e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2adb709cf55443398770778797eb487d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5351bc2c2552476aade6b753037fd29d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1ea86698ff44178f6141d2496fa847":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10322bbda984487d94461eed640b56e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"346beed676ea49e78776a32afa5fc5f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4d0c1c6aeb6483581a4cbc452eda020":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_776338a155e942bc924cb0cd9d1a987c","IPY_MODEL_9f5e7b7351284727ac09908c4f99f24d","IPY_MODEL_bd88e217aafd48cfa075a08fcf4d7e58"],"layout":"IPY_MODEL_39b1486749f448cf92f06f722cb95d4f"}},"776338a155e942bc924cb0cd9d1a987c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee3137fb71994aa0a647a9b664730e44","placeholder":"​","style":"IPY_MODEL_fb3e9da826794a5b8f4f980c4e6dfe33","value":"Downloading: 100%"}},"9f5e7b7351284727ac09908c4f99f24d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfee85d9796449d99be7a050f8229773","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78b4863e77c64df7b3cfa392e2018f82","value":466062}},"bd88e217aafd48cfa075a08fcf4d7e58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fea7c17ee18439183c5fec3c1207aec","placeholder":"​","style":"IPY_MODEL_3768ad8d4f384d33ad6ddfc8eb476ef9","value":" 466k/466k [00:00&lt;00:00, 3.15MB/s]"}},"39b1486749f448cf92f06f722cb95d4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee3137fb71994aa0a647a9b664730e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb3e9da826794a5b8f4f980c4e6dfe33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfee85d9796449d99be7a050f8229773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78b4863e77c64df7b3cfa392e2018f82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fea7c17ee18439183c5fec3c1207aec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3768ad8d4f384d33ad6ddfc8eb476ef9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}